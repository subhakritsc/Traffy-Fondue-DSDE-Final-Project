{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11699155,"sourceType":"datasetVersion","datasetId":7343235}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"f065c9d2-1032-494c-b7fc-c463631d8747","_cell_guid":"47adba91-9830-471d-8b40-3ddefb61cdf6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T11:42:52.420855Z","iopub.execute_input":"2025-05-06T11:42:52.421698Z","iopub.status.idle":"2025-05-06T11:42:54.271416Z","shell.execute_reply.started":"2025-05-06T11:42:52.421663Z","shell.execute_reply":"2025-05-06T11:42:54.270558Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ติดตั้ง Java 8 และ Spark\n!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n!wget -q https://dlcdn.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\n!tar xf spark-3.5.5-bin-hadoop3.tgz\n!mv spark-3.5.5-bin-hadoop3 spark\n\n# ติดตั้ง findspark\n!pip install -q findspark\n\n# ตั้งค่าตัวแปรสภาพแวดล้อม\nimport os\nimport sys\n\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\nos.environ[\"SPARK_HOME\"] = \"/kaggle/working/spark\"\nos.environ[\"PYTHONPATH\"] = \"/kaggle/working/spark/python:/kaggle/working/spark/python/lib\"\n\nsys.path.append(\"/kaggle/working/spark/python\")\nsys.path.append(\"/kaggle/working/spark/python/lib\")\n\n# เริ่มใช้งาน Spark\nimport findspark\nfindspark.init()\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\n\n\n# สร้าง Spark session\nspark = SparkSession.builder.master(\"local[*]\").appName(\"MySparkApp\").getOrCreate()\nspark\n\n","metadata":{"_uuid":"0023aa19-45ad-4dc2-8daa-cc0277891a50","_cell_guid":"d8d8fdf2-506e-4968-8330-9322f07257da","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T11:51:29.762052Z","iopub.execute_input":"2025-05-06T11:51:29.762419Z","iopub.status.idle":"2025-05-06T11:51:57.194753Z","shell.execute_reply.started":"2025-05-06T11:51:29.762378Z","shell.execute_reply":"2025-05-06T11:51:57.192777Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cluster_info_path = '/kaggle/input/all-traffy-cluster/cluster_info_df.csv'\ndf_path = '/kaggle/input/all-traffy-cluster/df_with_clustering.csv'\ndf = spark.read.csv(df_path, header=True, inferSchema=True)\ncluster_info = spark.read.csv(cluster_info_path, header=True, inferSchema=True)","metadata":{"_uuid":"29d4ece4-2f2f-4dfc-b2c0-579c001c4a08","_cell_guid":"7c996658-ffb1-4f2d-9ce0-c5771335d896","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T12:40:29.465135Z","iopub.execute_input":"2025-05-06T12:40:29.465447Z","iopub.status.idle":"2025-05-06T12:40:34.531918Z","shell.execute_reply.started":"2025-05-06T12:40:29.465422Z","shell.execute_reply":"2025-05-06T12:40:34.530949Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ดูแถวที่มี cluster หรือ text_cluster เป็น null ก่อน concat\ndf.filter(\n    (F.col(\"cluster\").isNull() & (F.col(\"cluster\") != -1)) | (F.col(\"text_cluster\").isNull()& (F.col(\"cluster\") != -1))\n).select(\"cluster\", \"text_cluster\").show()\n\n# ดูแถวที่ concat แล้วมีรูปแบบไม่ถูกต้อง (ไม่มี '.')\ndf.withColumn(\n    \"cluster_id\", F.concat_ws('.', F.col(\"cluster\"), F.col(\"text_cluster\").cast(\"int\"))\n).filter(~F.col(\"cluster_id\").contains('_')).select(\"cluster\", \"text_cluster\", \"cluster_id\").show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T12:35:32.808778Z","iopub.execute_input":"2025-05-06T12:35:32.809293Z","iopub.status.idle":"2025-05-06T12:35:39.532680Z","shell.execute_reply.started":"2025-05-06T12:35:32.809241Z","shell.execute_reply":"2025-05-06T12:35:39.531570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\n# Create a unique row ID\ndf = df.withColumn(\"row_id\", F.monotonically_increasing_id())\n\n# Define a window for ordering\nwindow_spec = Window.orderBy(\"row_id\")\n\n# Assign a unique noise ID for rows with cluster_id == '-1' or text_cluster is null\ndf = df.withColumn(\n    \"cluster\",\n    F.when(\n        (F.col(\"cluster\") == \"-1\") | (F.col(\"text_cluster\").isNull()),\n        F.concat(F.lit(\"noise_\"), F.row_number().over(window_spec))\n    ).otherwise(F.col(\"cluster\"))\n)\n\n# Drop the helper column\ndf = df.drop(\"row_id\")\n","metadata":{"_uuid":"822ff100-c6a0-4bf2-a0d6-1763c4e64aa2","_cell_guid":"a82595b7-8eb5-406d-9f1c-8fecb91342d2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T12:41:14.043863Z","iopub.execute_input":"2025-05-06T12:41:14.044283Z","iopub.status.idle":"2025-05-06T12:41:14.122591Z","shell.execute_reply.started":"2025-05-06T12:41:14.044256Z","shell.execute_reply":"2025-05-06T12:41:14.121349Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql import functions as F\n\n# สมมติว่า df คือตารางของคุณ\ndf = df.withColumn(\"cluster_id\", F.concat_ws('_', F.col('cluster'), F.col('text_cluster').cast(\"int\")))\ndf = df.drop('cluster', 'text_cluster')\n\n# ตรวจสอบผลลัพธ์\n# df.show()","metadata":{"_uuid":"cbf6a7a8-b071-42cb-b5a8-e988b9a383a4","_cell_guid":"75032c94-72f3-4f7f-b598-46824a3128ae","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T12:41:22.060690Z","iopub.execute_input":"2025-05-06T12:41:22.061044Z","iopub.status.idle":"2025-05-06T12:41:29.859933Z","shell.execute_reply.started":"2025-05-06T12:41:22.061016Z","shell.execute_reply":"2025-05-06T12:41:29.858805Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cluster_info = cluster_info.withColumn(\"cluster_id\", F.concat_ws('_', F.col('cluster_id'), F.col('text_cluster').cast(\"int\")))\ncluster_info = cluster_info.drop('text_cluster')\n# cluster_info.show()","metadata":{"_uuid":"f581d39b-8951-467b-8638-af6fdafe7cde","_cell_guid":"92770469-c04a-421f-afb2-afe99e4bdb7d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T12:41:54.442928Z","iopub.execute_input":"2025-05-06T12:41:54.443244Z","iopub.status.idle":"2025-05-06T12:41:54.547070Z","shell.execute_reply.started":"2025-05-06T12:41:54.443219Z","shell.execute_reply":"2025-05-06T12:41:54.545971Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean_coords = df.groupBy(\"cluster_id\").agg(\n    F.mean(\"latitude\").alias(\"lat\"),\n    F.mean(\"longitude\").alias(\"lon\")\n)","metadata":{"_uuid":"1673e0c0-0cf3-4a8f-85f0-e8a00b96f0b2","_cell_guid":"44921211-4a8a-45bb-88f5-e8044f45fafb","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T12:41:58.542602Z","iopub.execute_input":"2025-05-06T12:41:58.543008Z","iopub.status.idle":"2025-05-06T12:41:58.577623Z","shell.execute_reply.started":"2025-05-06T12:41:58.542979Z","shell.execute_reply":"2025-05-06T12:41:58.576700Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cluster_info = cluster_info.join(mean_coords, on='cluster_id', how='right')","metadata":{"_uuid":"9bd7eeb1-1a58-43b4-bbd2-6bb6b859d6b1","_cell_guid":"ed630044-9c30-40f6-9245-85b7209bda01","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T12:42:02.693088Z","iopub.execute_input":"2025-05-06T12:42:02.693921Z","iopub.status.idle":"2025-05-06T12:42:02.712490Z","shell.execute_reply.started":"2025-05-06T12:42:02.693862Z","shell.execute_reply":"2025-05-06T12:42:02.711456Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql import functions as F\n\n# สร้าง mapping ของ comment แรกต่อ cluster_id ที่ไม่มีคำอธิบาย\nnoise_desc = df.filter(F.col(\"cluster_id\").startswith(\"noise_\")) \\\n    .groupBy(\"cluster_id\") \\\n    .agg(F.first(\"comment\", ignorenulls=True).alias(\"noise_comment\"))\n\n# สมมติ df_cluster คือ DataFrame ที่มี column cluster_id, cluster_desc, lat, lon\ndf_cluster_filled = cluster_info.join(\n    noise_desc,\n    on=\"cluster_id\",\n    how=\"left\"\n).withColumn(\n    \"cluster_desc\",\n    F.when(F.col(\"cluster_desc\").isNull(), F.col(\"noise_comment\"))\n     .otherwise(F.col(\"cluster_desc\"))\n).drop(\"noise_comment\")","metadata":{"_uuid":"ebd0479a-07e0-4d75-b13a-87502b737be7","_cell_guid":"d8cbb390-125f-4eb5-b010-7c5eab3c3ec4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T12:55:29.381527Z","iopub.execute_input":"2025-05-06T12:55:29.382027Z","iopub.status.idle":"2025-05-06T12:55:29.514568Z","shell.execute_reply.started":"2025-05-06T12:55:29.381998Z","shell.execute_reply":"2025-05-06T12:55:29.513438Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql import functions as F\n\n# หาจำนวนของแต่ละ organization ในแต่ละ cluster\ndf_organization_count = df.groupBy(\"cluster_id\", \"organization\").count()\n\n# เลือก organization ที่มีจำนวนมากที่สุดในแต่ละ cluster\ndf_max_organization = df_organization_count.withColumn(\n    \"rank\", F.row_number().over(Window.partitionBy(\"cluster_id\").orderBy(F.col(\"count\").desc()))\n).filter(F.col(\"rank\") == 1).drop(\"rank\", \"count\")\n\n# Join ผลลัพธ์กลับไปยัง df_cluster_filled\ndf_cluster_filled = df_cluster_filled.join(\n    df_max_organization.select(\"cluster_id\", \"organization\"),\n    on=\"cluster_id\",\n    how=\"left\"\n)\n\n# แสดงผลลัพธ์\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T12:55:55.012537Z","iopub.execute_input":"2025-05-06T12:55:55.012828Z","iopub.status.idle":"2025-05-06T12:56:13.974171Z","shell.execute_reply.started":"2025-05-06T12:55:55.012807Z","shell.execute_reply":"2025-05-06T12:56:13.972802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.coalesce(1).write.option(\"header\", \"true\").csv(\"/kaggle/working/df_with_cluster_id\")\ndf_cluster_filled.coalesce(1).write.option(\"header\", \"true\").csv(\"/kaggle/working/cluster_information\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T12:47:27.447702Z","iopub.execute_input":"2025-05-06T12:47:27.448084Z","iopub.status.idle":"2025-05-06T12:48:07.039627Z","shell.execute_reply.started":"2025-05-06T12:47:27.448058Z","shell.execute_reply":"2025-05-06T12:48:07.037609Z"}},"outputs":[],"execution_count":null}]}