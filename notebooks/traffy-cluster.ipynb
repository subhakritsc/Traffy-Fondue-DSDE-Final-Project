{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11337738,"sourceType":"datasetVersion","datasetId":7092619},{"sourceId":11690173,"sourceType":"datasetVersion","datasetId":7337372}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"3685bbdb-5d03-4d03-a15b-c571431e23e9","_cell_guid":"7fbd8a27-80f2-46fe-bd30-59d913a1d210","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-07T05:24:47.997505Z","iopub.execute_input":"2025-05-07T05:24:47.997716Z","iopub.status.idle":"2025-05-07T05:24:49.215431Z","shell.execute_reply.started":"2025-05-07T05:24:47.997689Z","shell.execute_reply":"2025-05-07T05:24:49.214728Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"/kaggle/input/bangkok-traffy/bangkok_traffy.csv\n/kaggle/input/traffy/bangkok_traffy.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install pythainlp --upgrade\n!pip install -U hdbscan\n!pip install python-crfsuite","metadata":{"_uuid":"916907b3-00d0-40f2-9c81-e688143105d1","_cell_guid":"19bb95da-de4d-423c-ba59-ca0aa9d5344b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-07T05:24:49.216224Z","iopub.execute_input":"2025-05-07T05:24:49.216614Z","iopub.status.idle":"2025-05-07T05:25:00.908463Z","shell.execute_reply.started":"2025-05-07T05:24:49.216595Z","shell.execute_reply":"2025-05-07T05:25:00.907340Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Collecting pythainlp\n  Downloading pythainlp-5.1.1-py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from pythainlp) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->pythainlp) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->pythainlp) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->pythainlp) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->pythainlp) (2025.1.31)\nDownloading pythainlp-5.1.1-py3-none-any.whl (19.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pythainlp\nSuccessfully installed pythainlp-5.1.1\nCollecting hdbscan\n  Downloading hdbscan-0.8.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\nRequirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.26.4)\nRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.15.2)\nRequirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.2.2)\nRequirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.4.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.20->hdbscan) (2.4.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20->hdbscan) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.20->hdbscan) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.20->hdbscan) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.20->hdbscan) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.20->hdbscan) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.20->hdbscan) (2024.2.0)\nDownloading hdbscan-0.8.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: hdbscan\nSuccessfully installed hdbscan-0.8.40\nCollecting python-crfsuite\n  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: python-crfsuite\nSuccessfully installed python-crfsuite-0.9.11\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from sklearn.cluster import DBSCAN","metadata":{"_uuid":"f86c6102-6761-46fc-bf02-48f42279b221","_cell_guid":"bc907b7b-b279-4c93-a35c-5c63640b21a2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-07T05:25:00.910588Z","iopub.execute_input":"2025-05-07T05:25:00.910909Z","iopub.status.idle":"2025-05-07T05:25:01.748403Z","shell.execute_reply.started":"2025-05-07T05:25:00.910886Z","shell.execute_reply":"2025-05-07T05:25:01.747890Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/bangkok-traffy/bangkok_traffy.csv' ,quotechar='\"', on_bad_lines='skip')","metadata":{"_uuid":"dc4998f7-d704-48f5-87f1-4f7d1062d934","_cell_guid":"209a95e7-aef0-4dcf-93ff-5c17c5432fac","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-07T05:25:01.749013Z","iopub.execute_input":"2025-05-07T05:25:01.749293Z","iopub.status.idle":"2025-05-07T05:25:19.244596Z","shell.execute_reply.started":"2025-05-07T05:25:01.749276Z","shell.execute_reply":"2025-05-07T05:25:19.243867Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3284918269.py:1: DtypeWarning: Columns (12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv('/kaggle/input/bangkok-traffy/bangkok_traffy.csv' ,quotechar='\"', on_bad_lines='skip')\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"df = df.dropna(subset=['comment'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T08:40:12.341415Z","iopub.execute_input":"2025-05-06T08:40:12.341650Z","iopub.status.idle":"2025-05-06T08:40:12.553107Z","shell.execute_reply.started":"2025-05-06T08:40:12.341634Z","shell.execute_reply":"2025-05-06T08:40:12.552517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['comment'].isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T08:40:12.553855Z","iopub.execute_input":"2025-05-06T08:40:12.554067Z","iopub.status.idle":"2025-05-06T08:40:12.609560Z","shell.execute_reply.started":"2025-05-06T08:40:12.554051Z","shell.execute_reply":"2025-05-06T08:40:12.608913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.dropna(subset=['latitude', 'longitude'])\n\ncoords = df[['latitude', 'longitude']].to_numpy()\ncoords_rad = np.radians(coords)\nepsilon = 0.015 / 6371  # ประมาณ 15 เมตร\ndb = DBSCAN(eps=epsilon, min_samples=3, algorithm='ball_tree', metric='haversine')\ndf['cluster'] = db.fit_predict(coords_rad)","metadata":{"_uuid":"15eaa121-f2d9-485b-b044-59bac6536145","_cell_guid":"e80660c9-c6d4-471f-984d-31f0f9ac5f8e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T08:40:12.610369Z","iopub.execute_input":"2025-05-06T08:40:12.610638Z","iopub.status.idle":"2025-05-06T08:40:32.469458Z","shell.execute_reply.started":"2025-05-06T08:40:12.610615Z","shell.execute_reply":"2025-05-06T08:40:32.468838Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cluster_ids = (df['cluster'].max())\ncluster_ids","metadata":{"_uuid":"bf4802e8-d332-4dee-a08f-78d24ec865a3","_cell_guid":"11d496c9-361c-462d-ba65-f83939ec8af8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T08:40:32.500815Z","iopub.execute_input":"2025-05-06T08:40:32.501042Z","iopub.status.idle":"2025-05-06T08:40:32.513964Z","shell.execute_reply.started":"2025-05-06T08:40:32.501027Z","shell.execute_reply":"2025-05-06T08:40:32.513113Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_with_clustering = df[df['cluster'] > -1]","metadata":{"_uuid":"ac12a249-d53b-4cf9-b78c-9013e70db571","_cell_guid":"a0f6f7bd-18a5-47bf-b769-aa0a3e33ae85","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T08:40:32.545626Z","iopub.execute_input":"2025-05-06T08:40:32.545907Z","iopub.status.idle":"2025-05-06T08:40:32.687019Z","shell.execute_reply.started":"2025-05-06T08:40:32.545888Z","shell.execute_reply":"2025-05-06T08:40:32.686434Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_with_clustering['cluster'].value_counts()","metadata":{"_uuid":"129a6b4e-98ff-4575-8518-5f76387190bc","_cell_guid":"17192139-d02d-46a4-97a8-6511b2b6368f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T08:40:32.687786Z","iopub.execute_input":"2025-05-06T08:40:32.687992Z","iopub.status.idle":"2025-05-06T08:40:32.698997Z","shell.execute_reply.started":"2025-05-06T08:40:32.687977Z","shell.execute_reply":"2025-05-06T08:40:32.698359Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport string\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n# สำหรับภาษาไทย\nfrom pythainlp.tokenize import word_tokenize as thai_word_tokenize\nfrom pythainlp.corpus import thai_stopwords\n\n# โหลด NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')\n\nremove_list = [\n    'นะคะ', 'คะ', 'ค่ะ', 'ครับ', 'คับ', 'นะครับ', 'ช่วย', 'ช่วยด้วย', 'แจ้ง', \n    'ขอ', 'กรุณา', 'ความ', 'ขอความกรุณา', 'รายละเอียด', 'สวัสดี', 'รบกวน', 'หน้า', \n    'เบื้องต้น',  'ทำ', 'ขอให้', 'ดำเนินการ', 'ตรวจสอบ', 'โดยรวม', 'อีกด้วย', \n    'ขนาดใหญ่', 'คอย', 'ขอบพระคุณ', 'ขอบคุณ', 'พิจารณา', 'ทำให้เกิด', 'ระยะเวลา', \n    'นาน', 'บ่อย', 'เริ่ม', 'ติดต่อ', 'หน่วยงาน', 'ผม', \n    'ฉัน', 'ดิฉัน', 'ตอนนี้', 'เรื่อง','ต้องการ','เป็นเวลา','เวลา','แบบ',\n    'แบบนี้','นี้','ท่าน','คุณ','ต้องการ','ความช่วยเหลือ','ช่วยเหลือ','สอบถาม','ถาม','ความคืบหน้า',\n    'ชัชชาติ','traffyfondue','team','ร้องเรียน','ทุกครั้งที่','ทุกครั้ง','นิด','นิดนึง','นิดหน่อย','เล็กน้อย',\n    'นั้น','นี้','ตอนนี้','ตอนน้ัน'\n    \n]\n\n\ndef clean_sentence(sentence, remove_stopwords=False):\n  \n    # Convert to lowercase (เฉพาะภาษาอังกฤษ)\n    sentence = sentence.lower()\n    \n    # Remove HTML tags\n    sentence = re.sub(r'<.*?>', '', sentence)\n\n    # Remove URLs\n    sentence = re.sub(r'http\\S+|www\\S+|https\\S+', '', sentence)\n\n    # Remove email addresses\n    sentence = re.sub(r'\\S*@\\S*', '', sentence)\n\n\n    # Remove special characters and numbers (แต่เก็บสระไทยไว้)\n    sentence = re.sub(r'[^a-zA-Zก-ฮะ-์\\s]', '', sentence)\n\n    # Remove extra whitespace\n    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n\n\n    word_tokens = thai_word_tokenize(sentence)\n\n    \n    # Remove stopwords\n    if remove_stopwords:\n        stop_words = set(thai_stopwords())\n\n\n        word_tokens = [word for word in word_tokens if (word not in stop_words) and (word not in remove_list)]\n    \n    # รวมคำกลับเป็นประโยค\n    sentence = ' '.join(word_tokens)\n    \n    return sentence\n\n\n# Function to process a batch of sentences\ndef clean_sentences(sentences, remove_stopwords=False):\n    \"\"\"\n    Clean a list of sentences\n    \n    Args:\n        sentences (list): List of input sentences\n        remove_stopwords (bool): Whether to remove stopwords\n        lang (str): Language code (\"en\" for English, \"th\" for Thai)\n        \n    Returns:\n        list: List of cleaned sentences\n    \"\"\"\n    sentences = [str(s) for s in sentences]\n\n    return [clean_sentence(sentence, remove_stopwords) for sentence in sentences]","metadata":{"_uuid":"46485bd6-20d2-4ff8-b013-1ea9e68133ed","_cell_guid":"90c27095-5700-418e-a447-a4948b872143","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T08:40:32.699804Z","iopub.execute_input":"2025-05-06T08:40:32.700081Z","iopub.status.idle":"2025-05-06T08:40:32.713458Z","shell.execute_reply.started":"2025-05-06T08:40:32.700057Z","shell.execute_reply":"2025-05-06T08:40:32.712745Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom pythainlp.tag import NER\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nembedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\nner = NER(\"thainer\")","metadata":{"_uuid":"8e97a8e6-d370-49d3-b304-1693878ccdf8","_cell_guid":"57d85006-9a0c-48a8-be26-8453579e11ab","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T08:40:32.741452Z","iopub.execute_input":"2025-05-06T08:40:32.741717Z","iopub.status.idle":"2025-05-06T08:40:35.465107Z","shell.execute_reply.started":"2025-05-06T08:40:32.741695Z","shell.execute_reply":"2025-05-06T08:40:35.464414Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef get_representative_text(texts, embeddings):\n    centroid = np.mean(embeddings, axis=0)\n    sims = cosine_similarity([centroid], embeddings)[0]\n    return texts[np.argmax(sims)]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T08:40:35.475253Z","iopub.execute_input":"2025-05-06T08:40:35.475553Z","iopub.status.idle":"2025-05-06T08:40:35.486879Z","shell.execute_reply.started":"2025-05-06T08:40:35.475506Z","shell.execute_reply":"2025-05-06T08:40:35.486139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import AgglomerativeClustering\nimport hdbscan\nimport numpy as np\nfrom tqdm import tqdm\n\n\ncluster_list = []\n\nclustering_model = AgglomerativeClustering(\n        metric='precomputed',\n        linkage='average',\n        distance_threshold=0.5,  # หรือค่าที่คุณต้องการ\n        n_clusters=None  # ต้องตั้งเป็น None ถ้าใช้ distance_threshold\n    )\n\nfor target_cluster in tqdm(df_with_clustering['cluster'].unique()):\n\n\n    target_df = df_with_clustering[df_with_clustering['cluster'] == target_cluster]\n    target_comments = target_df['comment'].values.tolist()\n\n    # ข้อความตัวอย่าง\n    texts = target_comments  # หรือจะใส่ข้อความ list ก็ได้\n\n\n    type_cleaned = target_df['type'].map(lambda e: \" \".join([s.strip().strip(\"'\") for s in str(e)[1:-1].split(',')] if pd.notnull(e) else [])).values.tolist()\n\n\n    \n    # --------- Step 1: Sentence Embedding ---------\n    # model = SentenceTransformer('all-MiniLM-L6-v2')\n    embeddings = embedding_model.encode(texts, show_progress_bar=False)\n    \n    clean_texts = clean_sentences(texts,True)\n    type_embedding = embedding_model.encode(type_cleaned, show_progress_bar=False)\n    \n    # --------- Step 2: Named Entity Recognition ---------\n    \n    def extract_named_entities(text):\n        entities = ner.tag(text)\n        named_entities = [word for word, tag in entities if tag != 'O']\n        \n        # หากไม่มี named entities ให้คืนค่าเป็นคำที่เป็นข้อความดั้งเดิม\n        if not named_entities:\n            return text  # หรืออาจจะคืนค่าที่เป็น string ว่างก็ได้\n        return \" \".join(named_entities)\n    \n    \n    \n    ner_features = [extract_named_entities(text) for text in clean_texts]\n\n\n    # --------- Step 3: TF-IDF Vectorization of NER ---------\n    if len(ner_features) == 0:\n        print(\"NER features are empty after filtering!\")\n    else:\n        # --------- Step 3: TF-IDF Vectorization of NER ---------\n        vectorizer = TfidfVectorizer()\n        try:\n            ner_tfidf = vectorizer.fit_transform(ner_features).toarray()\n        except ValueError:\n            continue  # ข้ามกรณี vocabulary ว่าง\n    \n    \n    # --------- Step 4: Combine Embedding + NER ---------\n   \n    combined_features = np.concatenate([embeddings,type_embedding,ner_tfidf], axis=1)\n    \n    \n    \n    \n    # --------- Step 5: HDBSCAN Clustering ---------\n    similarity_matrix = cosine_similarity(combined_features)\n    \n    similarity_matrix = similarity_matrix.astype(np.float64)\n    distance_matrix = 1 - similarity_matrix\n\n    \n\n    \n    \n    labels = clustering_model.fit_predict(distance_matrix)  # ถ้าใช้ similarity ต้องแปลงเป็นระยะทาง\n    df.loc[target_df.index, 'text_cluster'] = labels\n    \n    # --------- Step 6: Show results ---------\n    from collections import defaultdict\n    \n    clustered_texts = defaultdict(list)\n    \n    for label, text in zip(labels, texts):\n        clustered_texts[label].append(text)\n\n    for sub_label in set(labels):\n        # เลือก index ของข้อความในคลัสเตอร์ย่อย\n        sub_indices = [i for i, l in enumerate(labels) if l == sub_label]\n        \n        sub_texts = [texts[i] for i in sub_indices]\n        sub_embeddings = embeddings[sub_indices]  # เลือกจาก embedding เดิมที่ encode ไปแล้ว\n        \n        rep = get_representative_text(sub_texts, sub_embeddings)\n        \n        cluster_list.append({\n            'cluster_id': target_cluster,\n            'text_cluster': sub_label,\n            'cluster_desc': rep\n        })\n","metadata":{"_uuid":"e51a1afc-2cf8-47f5-8d00-122ff6b02fca","_cell_guid":"81a98418-492b-425b-a4fa-c30f4580d13d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-06T08:40:35.487811Z","iopub.execute_input":"2025-05-06T08:40:35.488076Z","execution_failed":"2025-05-06T08:40:57.095Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result = pd.DataFrame(cluster_list)","metadata":{"_uuid":"fa0efef3-87d8-4ec9-bc07-a31e9d8a21cb","_cell_guid":"71a4710b-6db7-4db3-9ffa-d32bf550208b","trusted":true,"collapsed":false,"execution":{"execution_failed":"2025-05-06T08:40:57.095Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result.to_csv(\"cluster_info_df.csv\", index=False)\ndf.to_csv(\"df_with_clustering.csv\", index=False)","metadata":{"_uuid":"61dff5a7-5f2a-4d57-b73a-a52b9c51d98c","_cell_guid":"d73ec482-1785-4458-9ebd-df5da19646d0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-05-06T08:40:57.095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n\nmodel = AgglomerativeClustering(\n    metric='precomputed',\n    linkage='average',\n    distance_threshold=0.5,  # หรือค่าที่คุณต้องการ\n    n_clusters=None  # ต้องตั้งเป็น None ถ้าใช้ distance_threshold\n)\n\nlabels = model.fit_predict(distance_matrix)  # ถ้าใช้ similarity ต้องแปลงเป็นระยะทาง\n\n# --------- Step 6: Show results ---------\nfrom collections import defaultdict\n\nclustered_texts = defaultdict(list)\n\nfor label, text in zip(labels, texts):\n    clustered_texts[label].append(text)\n\n# แสดงผลเป็นกลุ่ม\nfor label, group in clustered_texts.items():\n    print(f\"\\n[Cluster {label}]\")\n    # print(len(group))\n    for t in group:\n        print(f\" - {t}\")","metadata":{"_uuid":"7c703f32-cc4b-43ac-86df-4b8a1b067c67","_cell_guid":"4ed57d25-b894-4016-b7b6-467d3b25386d","trusted":true,"collapsed":false,"execution":{"execution_failed":"2025-05-06T08:40:57.095Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}