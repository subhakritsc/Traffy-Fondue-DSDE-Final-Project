{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tWOGXdlY3PGC"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H3bhONdJ1A_x"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Tt811-u-1Bkt"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "    !wget -q https://dlcdn.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\n",
        "    !tar xf spark-3.5.5-bin-hadoop3.tgz\n",
        "    !mv spark-3.5.5-bin-hadoop3 spark\n",
        "    !pip install -q findspark\n",
        "    import os\n",
        "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    os.environ[\"SPARK_HOME\"] = \"/content/spark\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_UgimjOw1Egf"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "spark_url = 'local'\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "spark = SparkSession.builder\\\n",
        "        .master(spark_url)\\\n",
        "        .appName('Spark')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsilw1hS4JAP"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGf9uyjc1PFW",
        "outputId": "cca90163-42e3-4e76-f7fe-abf2ac9af5d2"
      },
      "outputs": [],
      "source": [
        "# Loading File from Google Drive\n",
        "!gdown 'https://drive.google.com/uc?id=1PE-fNRsLA9qqEHmaiof0xV-Uz_5ZfCuO' --output df_with_cluster.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Hm3C6jXOzJj1"
      },
      "outputs": [],
      "source": [
        "# -------------- TABLE 1: Showing Example Comment ---------------------\n",
        "\n",
        "# 1. Read the CSV\n",
        "df_with_clustering = spark.read.csv('df_with_cluster.csv', header=True, inferSchema=True)\n",
        "\n",
        "# 2. Select and rename\n",
        "df = df_with_clustering.select(\n",
        "    col('cluster_id').alias('cluster'),\n",
        "    col('comment')\n",
        ")\n",
        "\n",
        "# 3. Drop noise row\n",
        "df = df.filter(~col('cluster').startswith('noise'))\n",
        "\n",
        "# 4. Save the DataFrame as a single CSV file\n",
        "df.coalesce(1).write.csv('example_comment', header=True, mode='overwrite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epORc3yB_ZZA"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcjZPjwG_YLq",
        "outputId": "06682600-383b-41f9-c4ad-305577a61c9b"
      },
      "outputs": [],
      "source": [
        "!gdown 'https://drive.google.com/uc?id=12RR873XXJrFnJiKJWkqYNnzAfUB-sDyL' --output bright.csv\n",
        "!gdown 'https://drive.google.com/uc?id=1oAjRZe9Mq80AyLi5G2HSwsXxSKY8mZ2C' --output piti.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOnhF30gzU16",
        "outputId": "7e0593ae-275f-47e1-ae40-b296a04840b2"
      },
      "outputs": [],
      "source": [
        "# -------------- TABLE 2: Map and Graph visualization ---------------------\n",
        "\n",
        "# 1. piti: 'cluster_id', 'num_times', 'status'\n",
        "# 2. bright: 'cluster_id', 'cluster_desc', 'lat', 'long', 'organization'\n",
        "# 3. keen: zone scraping to get 'zone'\n",
        "\n",
        "# 1. Load the pre-trained KNN model from Google Drive\n",
        "!gdown --fuzzy \"https://drive.google.com/file/d/1w21nVLe-xYaQ3ZiP5nOY_f9R_t9dMbII/view?usp=drive_link\"\n",
        "\n",
        "with open(\"knn.pkl\", \"rb\") as f:\n",
        "    knn = pickle.load(f)\n",
        "\n",
        "# 2. Read the CSV\n",
        "piti_df = spark.read.csv('piti.csv', header=True, inferSchema=True)\n",
        "bright_df = spark.read.csv('bright.csv', header=True, inferSchema=True)\n",
        "bright_df = bright_df.withColumnRenamed(\"lon\", \"long\")\n",
        "bright_df = bright_df.filter(~col('cluster_id').startswith('noise'))\n",
        "\n",
        "# 3. Merge the tables on 'cluster_id'\n",
        "merged_df = piti_df.join(bright_df, on='cluster_id', how='inner')\n",
        "\n",
        "# 4. Define the pandas_udf function to apply the KNN model and predict the zone\n",
        "@pandas_udf(StringType())\n",
        "def predict_zone_pandas_udf(lat: pd.Series, lon: pd.Series) -> pd.Series:\n",
        "    # Assuming knn is already loaded with pickle\n",
        "    coords = np.column_stack((lat.values, lon.values))  # Combine lat and lon into a 2D array\n",
        "    return pd.Series(knn.predict(coords))\n",
        "\n",
        "# 5. Apply the pandas_udf function to predict the zone based on latitude and longitude\n",
        "merged_df = merged_df.withColumn('zone', predict_zone_pandas_udf(col('lat'), col('long')))\n",
        "\n",
        "# 6. Save the DataFrame as a single CSV file\n",
        "merged_df.coalesce(1).write.csv('cluster_data', header=True, mode='overwrite')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
